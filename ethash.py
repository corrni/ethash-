import copy
from utils import isprime, sha3_512, sha3_256, xor, \
    serialize_hash, zpad, encode_int
from constants import FNV_PRIME, CACHE_BYTES_INIT, CACHE_BYTES_GROWTH, \
    EPOCH_LENGTH, HASH_BYTES, MIX_BYTES, DATASET_BYTES_GROWTH, \
    DATASET_PARENTS, DATASET_BYTES_INIT, ACCESSES, CACHE_ROUNDS, WORD_BYTES

# -- Parameters for Ethash's cache & dataset --
#   - get_cache_size
#   - get_full_size


def get_cache_size(block_number):
    sz = CACHE_BYTES_INIT + CACHE_BYTES_GROWTH * (block_number // EPOCH_LENGTH)
    sz -= HASH_BYTES
    while not isprime(sz / HASH_BYTES):
        sz -= 2 * HASH_BYTES
    return sz


def get_full_size(block_number):
    sz = DATASET_BYTES_INIT + DATASET_BYTES_GROWTH * (
        block_number // EPOCH_LENGTH)
    sz -= MIX_BYTES
    while not isprime(sz / MIX_BYTES):
        sz -= 2 * MIX_BYTES
    return sz


# -- Cache Generation --
#   - mkcache


def mkcache(cache_size, seed):
    """Cache generation.
        The cache production process involves first sequentially filling up
        32 MB of memory, then performing two passes of Sergio Demian Lerner's
        RandMemoHash algorithm from Strict Memory Hard Hashing Functions

        The output is a set of 524288 64-byte values.

        SEE: www.hashcash.org/papers/memohash.pdf
    """
    n = cache_size // HASH_BYTES

    # Sequentially produce initial dataset
    o = [sha3_512(seed)]
    for i in range(1, n):
        o.append(sha3_512(o[-1]))

    # Use a low-rounded version of randmemohash
    for _ in range(CACHE_ROUNDS):
        for i in range(n):
            v = o[i][0] % n
            o[i] = sha3_512(map(xor, o[(i - 1 + n) % n], o[v]))

    return o


# -- Data Aggregation Function --


def fnv(v1, v2):
    """Data aggregation function. Inspired by FNV hash.

        Although yellow paper specifies fnv as `v1*(FNV_PRIME ^ v2)`, current
        implementations consistently use `(v1 * FNV_PRIME) ^ v2) % 2**32`

        SEE: http://tinyurl.com/qxcqjhk
    """
    return ((v1 * FNV_PRIME) ^ v2) % 2**32


# -- Full dataset calculation --


# Computes each 64-byte item in full 1GB dataset
def calc_dataset_item(cache, i):
    n = len(cache)
    r = HASH_BYTES // WORD_BYTES
    # initialize the mix
    mix = copy.copy(cache[i % n])
    mix[0] ^= i
    mix = sha3_512(mix)

    # fnv it with a lot of random cache nodes based on i
    for j in range(DATASET_PARENTS):
        cache_index = fnv(i ^ j, mix[j % r])

        # Mix function
        mix = map(fnv, mix, cache[cache_index % n])

    return sha3_512(mix)


# [...] Essentially, we combine data from 256 pseudorandomly
# selected cache nodes, and hash that to compute the dataset node.
#
# The entire dataset is then generated by:
def calc_dataset(full_size, cache):
    return [
        calc_dataset_item(cache, i) for i in range(full_size // HASH_BYTES)
    ]


# -- Main Loop --
#   - hashimoto
#   - hashimoto_light
#   - hashimoto_full


def hashimoto(header, nonce, full_size, dataset_lookup):
    n = full_size / HASH_BYTES
    w = MIX_BYTES // WORD_BYTES
    mixhashes = MIX_BYTES / HASH_BYTES

    # combine header+nonce into a 64 byte seed
    s = sha3_512(header + nonce[::-1])

    # start the mix with replicated s
    mix = []
    for _ in range(MIX_BYTES / HASH_BYTES):
        mix.extend(s)

    # mix in random dataset nodes
    # NOTE: this is the expensive bit
    for i in range(ACCESSES):
        p = fnv(i ^ s[0], mix[i % w]) % (n // mixhashes) * mixhashes
        newdata = []
        for j in range(MIX_BYTES / HASH_BYTES):
            newdata.extend(dataset_lookup(p + j))
        mix = map(fnv, mix, newdata)

    # compress mix
    cmix = []
    for i in range(0, len(mix), 4):
        cmix.append(fnv(fnv(fnv(mix[i], mix[i + 1]), mix[i + 2]), mix[i + 3]))
    return {
        "mix digest": serialize_hash(cmix),
        "result": serialize_hash(sha3_256(s + cmix))
    }


def hashimoto_light(full_size, cache, header, nonce):
    return hashimoto(header, nonce, full_size,
                     lambda x: calc_dataset_item(cache, x))


def hashimoto_full(full_size, dataset, header, nonce):
    return hashimoto(header, nonce, full_size, lambda x: dataset[x])


# -- Mining Algorithm --


def mine(full_size, dataset, header, difficulty):
    from random import randint

    # computing difficulty target
    target = zpad(encode_int(2**256 // difficulty), 64)[::-1]

    # nonces are randomly generated
    nonce = randint(0, 2**64)

    while hashimoto_full(full_size, dataset, header, nonce) > target:
        nonce = (nonce + 1) % 2**64

    return nonce


# Defining Seed Hash
# NOTE: used to mine on top of a given block
def get_seedhash(block):
    s = '\x00' * 32
    for i in range(block.number // EPOCH_LENGTH):
        s = serialize_hash(sha3_256(s))
    return s
